"""Step to evaluate generated clinical responses using LLM-as-a-Judge with binary questions."""

import re
from typing import Dict, Any, Annotated
from zenml import step
from zenml.logger import get_logger

logger = get_logger(__name__)


def create_judge_prompt(question: str, generated_response: str, retrieved_context: str) -> str:
    """
    Create a structured prompt for LLM judge evaluation.
    
    Args:
        question: Original clinical question
        generated_response: RAG system's generated response
        retrieved_context: Retrieved document chunks used for generation
        
    Returns:
        Formatted prompt for LLM judge
    """
    prompt = f"""You are a clinical expert evaluating a RAG system's response to a medical question.

ORIGINAL QUESTION:
{question}

GENERATED RESPONSE:
{generated_response}

RETRIEVED CONTEXT:
{retrieved_context}

Please evaluate this clinical RAG response by answering each question with "Yes" or "No":

CLINICAL ACCURACY:
1. Is the medical information factually correct? (Yes/No)
2. Are any treatment recommendations clinically appropriate? (Yes/No)
3. Would this information be safe for clinical decision-making? (Yes/No)

RELEVANCE:
4. Does this directly answer the clinical question asked? (Yes/No)
5. Would this response help the healthcare provider with their query? (Yes/No)

EVIDENCE SUPPORT:
6. Is each medical claim supported by the retrieved documents? (Yes/No)
7. Are the citations relevant and appropriate? (Yes/No)

Please provide your evaluation in this exact format:
1. [Yes/No]
2. [Yes/No]
3. [Yes/No]
4. [Yes/No]
5. [Yes/No]
6. [Yes/No]
7. [Yes/No]

Count the total "Yes" answers and provide your final score:
SCORE: [X]/7"""

    return prompt


def parse_judge_response(response: str) -> Dict[str, Any]:
    """
    Parse LLM judge response to extract individual answers and score.
    
    Args:
        response: Raw response from LLM judge
        
    Returns:
        Dictionary containing parsed results
    """
    try:
        # Extract individual Yes/No answers
        yes_no_pattern = r'\d+\.\s*(Yes|No)'
        matches = re.findall(yes_no_pattern, response, re.IGNORECASE)
        
        # Convert to boolean
        individual_scores = [match.lower() == 'yes' for match in matches]
        
        # Extract final score if provided
        score_pattern = r'SCORE:\s*(\d+)/7'
        score_match = re.search(score_pattern, response, re.IGNORECASE)
        
        if score_match:
            declared_score = int(score_match.group(1))
        else:
            declared_score = None
        
        # Calculate actual count of Yes answers
        actual_count = sum(individual_scores)
        
        # Validate that we got 7 answers
        if len(individual_scores) != 7:
            logger.warning(f"Expected 7 answers, got {len(individual_scores)}")
            # Pad with False if needed, truncate if too many
            individual_scores = (individual_scores + [False] * 7)[:7]
            actual_count = sum(individual_scores)
        
        # Check if declared score matches actual count
        score_consistent = declared_score == actual_count if declared_score is not None else True
        
        return {
            'individual_scores': individual_scores,
            'actual_count': actual_count,
            'declared_score': declared_score,
            'score_consistent': score_consistent,
            'final_score': actual_count,  # Use actual count as authoritative
            'raw_response': response,
            'parse_success': True
        }
        
    except Exception as e:
        logger.error(f"Failed to parse judge response: {e}")
        return {
            'individual_scores': [False] * 7,
            'actual_count': 0,
            'declared_score': None,
            'score_consistent': False,
            'final_score': 0,
            'raw_response': response,
            'parse_success': False,
            'parse_error': str(e)
        }


@step
def llm_judge_evaluation(
    question: str,
    generated_response: str,
    retrieved_context: str,
    judge_model: str = "openai/gpt-3.5-turbo",
    temperature: float = 0.0
) -> Annotated[Dict[str, Any], "judge_evaluation_result"]:
    """
    Evaluate generated clinical response using LLM-as-a-Judge with binary questions.
    
    Args:
        question: Original clinical question
        generated_response: Response generated by RAG system
        retrieved_context: Context used for generation
        judge_model: LiteLLM model name for judging
        temperature: Generation temperature for judge
        
    Returns:
        Dictionary containing evaluation results and scores
    """
    try:
        # Import litellm
        try:
            from litellm import completion
        except ImportError:
            return {
                'success': False,
                'error': "LiteLLM not available for judge evaluation",
                'final_score': 0,
                'individual_scores': [False] * 7
            }
        
        # Create judge prompt
        prompt = create_judge_prompt(question, generated_response, retrieved_context)
        
        # Call LLM judge
        logger.info(f"Calling LLM judge with model: {judge_model}")
        
        response = completion(
            model=judge_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=500
        )
        
        judge_response = response.choices[0].message.content
        
        # Parse response
        parsed_result = parse_judge_response(judge_response)
        
        # Add metadata
        result = {
            'success': True,
            'question': question,
            'judge_model': judge_model,
            'final_score': parsed_result['final_score'],
            'score_out_of': 7,
            'percentage': (parsed_result['final_score'] / 7) * 100,
            'individual_scores': parsed_result['individual_scores'],
            'parse_success': parsed_result['parse_success'],
            'score_consistent': parsed_result['score_consistent'],
            'raw_judge_response': parsed_result['raw_response'],
            'error': None
        }
        
        # Add dimension-specific scores
        if len(parsed_result['individual_scores']) == 7:
            result['dimension_scores'] = {
                'clinical_accuracy': sum(parsed_result['individual_scores'][0:3]),  # Questions 1-3
                'relevance': sum(parsed_result['individual_scores'][3:5]),          # Questions 4-5
                'evidence_support': sum(parsed_result['individual_scores'][5:7])    # Questions 6-7
            }
        
        if not parsed_result['parse_success']:
            result['parse_error'] = parsed_result.get('parse_error', 'Unknown parsing error')
        
        logger.info(f"Judge evaluation completed: {result['final_score']}/7 ({result['percentage']:.1f}%)")
        
        return result
        
    except Exception as e:
        logger.error(f"LLM judge evaluation failed: {e}")
        return {
            'success': False,
            'error': str(e),
            'final_score': 0,
            'score_out_of': 7,
            'percentage': 0.0,
            'individual_scores': [False] * 7,
            'dimension_scores': {
                'clinical_accuracy': 0,
                'relevance': 0,
                'evidence_support': 0
            }
        }